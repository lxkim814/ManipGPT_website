<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Robot Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?</h1>
          <span class="has-text-danger" style="font-weight: bold; font-size: 1.5rem; display: block; margin-bottom: 0.75rem;">
            IROS 2025
          </span>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=aQTkWjQAAAAJ">Taewhan Kim</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/1524101973">Hojin Bae</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Zeming_Li4">Zeming Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://clorislili.github.io/clorisLi/">Xiaoqi Li</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yaroslavponomarenko.github.io/">Iaroslav Ponomarenko</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://warshallrho.github.io/">Ruihai Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zsdonghao.github.io/">Hao Dong</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CFCS, School of Computer Science, Peking University,</span>
            <span class="author-block"><sup>2</sup>PKU-Agibot Lab</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.10050"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.10050"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!--/ Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual actionable affordance has emerged as a transformative approach in robotics, focusing on perceiving interaction areas prior to manipulation. Traditional methods rely on pixel sampling to identify successful interaction samples or processing pointclouds for affordance mapping. However, these approaches are computationally intensive and struggle to adapt to diverse and dynamic environments. This paper introduces ManipGPT, a framework designed to predict optimal interaction areas for articulated objects using a large pre-trained vision transformer (ViT). We created a dataset of 9.9k simulated and real images to bridge the visual sim-to-real gap and enhance real-world applicability. By fine-tuning the vision transformer on this small dataset, we significantly improved part-level affordance segmentation, adapting the modelâ€™s in-context segmentation capabilities to robot manipulation scenarios. This enables effective manipulation across simulated and real-world environments by generating part-level affordance masks, paired with an impedance adaptation policy, sufficiently eliminating the need for complex datasets or perception systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->




    
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
<!--           <iframe src="https://www.youtube.com/watch?v=e74AtDbeD0Q"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <iframe src="https://www.youtube.com/embed/e74AtDbeD0Q"
            frameborder="0"
            allow="autoplay; encrypted-media"
            allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<!-- Custom Figures and Text Section -->
<section class="section">
  <div class="container is-widescreen">
    <div class="columns is-centered">
      <div class="column column is-full">
        <h2 class="title is-3 has-text-centered">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Our method processes an RGB image with a visual prompt to generate an affordance mask, which determines the contact point and manipulation direction.
          </p>
          <figure class="has-text-centered">
            <img
              src="./static/images/teaser3.png"
              alt="Overview"
              style="max-width: 100%; height: auto;"
            >
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


  
  
<!-- Custom Figures and Text Section -->
<section class="section">
  <div class="container is-widescreen">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">System Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            Our method involves fine-tuning a vision transformer on part-level affordance masks, integrating it with an impedance controller for real-world manipulation. Below is the system pipeline of our approach.
          </p>
          <figure class="has-text-centered">
            <img
              src="./static/images/pipeline.png"
              alt="System Pipeline"
              style="max-width: 100%; height: auto;"
            >
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>



  
<!-- Title OUTSIDE the grey hero box -->
<section class="section">
  <div class="container has-text-centered">
    <h2 class="title is-3">Experiment Videos</h2>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
<!--     <div class="has-text-centered">
      <h2 class="title is-3">Experiment Videos</h2>
    </div> -->
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-box">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_box.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-bottle">
          <video poster="" id="bottle-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_bottle.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-trashcan">
          <video poster="" id="trashcan" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_trashcan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-stapler">
          <video poster="" id="stapler" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_stapler.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-dispenser">
          <video poster="" id="dispenser" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_dispenser.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-drawer">
          <video poster="" id="drawer" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/experiment_drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{kim2024manipgptaffordancesegmentationlarge,
      title={ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for Articulated Object Manipulation?}, 
      author={Taewhan Kim and Hojin Bae and Zeming Li and Xiaoqi Li and Iaroslav Ponomarenko and Ruihai Wu and Hao Dong},
      year={2024},
      eprint={2412.10050},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2412.10050}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content has-text-centered">
          Website template borrowed from 
          <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
